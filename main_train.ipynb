{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca340901-252e-4685-994c-92d3582a19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 23:33:31,283\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 5.363227683119476e-05,\n",
       " 'val_l1': 0.0036629082169383764,\n",
       " 'train_loss': 2.1686162654077634e-05,\n",
       " 'train_l1': 0.002796189161017537,\n",
       " 'epoch': 73,\n",
       " 'i_re_best': '0',\n",
       " 'timestamp': 1734230225,\n",
       " 'checkpoint_dir_name': 'checkpoint_000000',\n",
       " 'should_checkpoint': True,\n",
       " 'done': True,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'e5144_00142',\n",
       " 'date': '2024-12-15_03-37-05',\n",
       " 'time_this_iter_s': 444.7897574901581,\n",
       " 'time_total_s': 444.7897574901581,\n",
       " 'pid': 4145084,\n",
       " 'hostname': 'wolfgang-cpu04',\n",
       " 'node_ip': '192.168.8.24',\n",
       " 'config': {'lr': 0.001,\n",
       "  'batch_size': 32,\n",
       "  'n_layers_ref': 4,\n",
       "  'n_nodes_ref': 36,\n",
       "  'n_nodes_feature_ref': 36,\n",
       "  'n_features_ref': 8,\n",
       "  'n_features': 8,\n",
       "  'n_layers_parameter': 2,\n",
       "  'n_nodes_parameter': 82,\n",
       "  'n_nodes_entropy_feature': 16},\n",
       " 'time_since_restore': 444.7897574901581,\n",
       " 'iterations_since_restore': 1,\n",
       " 'experiment_tag': '142_batch_size=32,lr=0.0010,n_features=8,n_features_ref=8,n_layers_parameter=2,n_layers_ref=4,n_nodes_entropy_feature=16,n_nodes_feature_ref=36,n_nodes_parameter=82,n_nodes_ref=36',\n",
       " 'lr': 0.001,\n",
       " 'batch_size': 32,\n",
       " 'n_layers_ref': 4,\n",
       " 'n_nodes_ref': 36,\n",
       " 'n_nodes_feature_ref': 36,\n",
       " 'n_features_ref': 8,\n",
       " 'n_features': 8,\n",
       " 'n_layers_parameter': 2,\n",
       " 'n_nodes_parameter': 82,\n",
       " 'n_nodes_entropy_feature': 16,\n",
       " 'data_csv_train': '/data/work/ac135066/workspace/deep_entropy_2/data/vis_entropy_train_k7_deduped.csv',\n",
       " 'data_csv_val': '/data/work/ac135066/workspace/deep_entropy_2/data/vis_entropy_val_k7_deduped.csv',\n",
       " 'data_csv_test': '/data/work/ac135066/workspace/deep_entropy_2/data/vis_entropy_test_k7_deduped.csv',\n",
       " 'embedding_size': 7,\n",
       " 'build': 'model0_ci_norm2',\n",
       " 'model_path': '/data/work/ac135066/workspace/deep_entropy_2/train_model0_ci_norm22/train71/',\n",
       " 'checkpoint_path': '/data/work/ac135066/workspace/deep_entropy_2/train_model0_ci_norm22/train71/checkpoint.pt',\n",
       " 'n_entropy_features': 8,\n",
       " 'checkpoint_path_native': '/tmp/checkpoint_tmp_6c63bb88d4cf4d36b4cfbde1a8554281/checkpoint.pt',\n",
       " 'loss': {'training': 0.036177102570178477,\n",
       "  'validation': 0.06803802452896335,\n",
       "  'test': 0.2770408431239524},\n",
       " 'l1': {'training': 2.589245004579425,\n",
       "  'validation': 2.8426268552429974,\n",
       "  'test': 3.070628332643537}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import ray\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray import tune\n",
    "\n",
    "from deep_entropy_scaling.nn_dataset import dataset, minmax_to_json, minmax_from_json\n",
    "from deep_entropy_scaling.nn_training import train_looper, load_model0, best_model  # train_loop\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "def main(config, config_train_space):\n",
    "\n",
    "    y_features = [\"log_value\"]\n",
    "    data_train = dataset(config[\"data_csv_train\"], y_features=y_features,\n",
    "                         scaler_fit=True)\n",
    "    joblib.dump(data_train.scalerX, config[\"model_path\"]+'Xscaler.gz')\n",
    "    joblib.dump(data_train.scalerY, config[\"model_path\"]+'Yscaler.gz')\n",
    "    minmax_to_json(data_train.scalerX, config[\"model_path\"]+'Xscaler.json')\n",
    "    minmax_to_json(data_train.scalerY, config[\"model_path\"]+'Yscaler.json')    \n",
    "    del data_train\n",
    "\n",
    "\n",
    "    config_train_space = {\n",
    "        \"lr\": tune.choice(config_train_space[\"lr\"]),\n",
    "        \"batch_size\": tune.choice(config_train_space[\"batch_size\"]),\n",
    "        \"n_layers_ref\": tune.choice(config_train_space[\"n_layers_ref\"]),\n",
    "        \"n_nodes_ref\": tune.choice(config_train_space[\"n_nodes_ref\"]),\n",
    "        \"n_nodes_feature_ref\":\n",
    "        tune.choice(config_train_space[\"n_nodes_feature_ref\"]),\n",
    "        \"n_features_ref\": tune.choice(config_train_space[\"n_features_ref\"]),\n",
    "        \"n_features\": tune.choice(config_train_space[\"n_features\"]),\n",
    "        \"n_layers_parameter\":\n",
    "        tune.choice(config_train_space[\"n_layers_parameter\"]),\n",
    "        \"n_nodes_parameter\":\n",
    "        tune.choice(config_train_space[\"n_nodes_parameter\"]),\n",
    "        \"n_nodes_entropy_feature\":\n",
    "        tune.choice(config_train_space[\"n_nodes_entropy_feature\"]),\n",
    "        }\n",
    "\n",
    "    #def train_lambda(x): return train_looper(x, config)\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=1000,\n",
    "        grace_period=200,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_looper, config=config),\n",
    "            resources={\"cpu\": 6}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=1,\n",
    "        ),\n",
    "        param_space=config_train_space,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(\"val_l1\", \"min\", scope=\"all\")\n",
    "\n",
    "    print(\"Best trial config: {}\".format(best_result.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"val_loss\"]))\n",
    "    print(\"Best trial final validation MEAD: {}\".format(\n",
    "        best_result.metrics[\"val_l1\"]))\n",
    "\n",
    "    return best_result, config\n",
    "\n",
    "\n",
    "# ref training:\n",
    "config_train_space = {\n",
    "    \"lr\": [0.002, 0.001, 0.005],\n",
    "    \"batch_size\": [64, 128, 256],\n",
    "\n",
    "    \"n_layers_ref\": [4, 6, 8],\n",
    "    \"n_nodes_ref\": [24, 36, 48],\n",
    "    \"n_nodes_feature_ref\": [12, 16],\n",
    "    \n",
    "    \"n_features_ref\": [8, 12],\n",
    "    \"n_features\": [8, 12],\n",
    "\n",
    "    \"n_layers_parameter\": [4, 6, 8],\n",
    "    \"n_nodes_parameter\": [36, 48, 64],\n",
    "    \"n_nodes_entropy_feature\": [12, 16, 24],\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "THE DATA IS NOT PUBLISHED.\n",
    "THEREFORE YOU WONT BE ABLE TO RUN THIS NOTEBOOK WITHOUT ERRORS\n",
    "\n",
    "USE YOUR OWN DATA TO TRAIN!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "config = {}\n",
    "dpath = \"/home/derwer/Desktop/PHD/deep_entropy_2/data_preparation/merged_data/\"\n",
    "config[\"data_csv_train\"] = dpath + \"vis_test_alk_deduped.csv\"\n",
    "config[\"data_csv_val\"] = dpath + \"vis_test_alk_deduped.csv\"\n",
    "config[\"data_csv_test\"] = dpath + \"vis_test_alk_deduped.csv\"\n",
    "\n",
    "\n",
    "config[\"embedding_size\"] = 7\n",
    "config[\"model_path\"] = \"train0/\"\n",
    "\n",
    "\"\"\"\n",
    "uncomment if you wanna train ur own model:\n",
    "\"\"\"\n",
    "train = False\n",
    "if train:\n",
    "    best_result, config = main(config, config_train_space)\n",
    "\n",
    "path = \"train71/\"\n",
    "\n",
    "with open(path+\"best_result_config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa9db45-d85c-4dcc-b08d-cb14e0182d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# config[\"data_csv_test\"] = dpath + \"vis_entropy_test_deduped.csv\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mbest_result\u001b[49m\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mto_directory(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m final_result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_result\u001b[38;5;241m.\u001b[39mmetrics, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_result\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig,\n\u001b[1;32m      4\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: cname,\n\u001b[1;32m      5\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_entropy_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_result\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m      6\u001b[0m model, data_train \u001b[38;5;241m=\u001b[39m best_model(final_result, exp_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_result' is not defined"
     ]
    }
   ],
   "source": [
    "# config[\"data_csv_test\"] = dpath + \"vis_entropy_test_deduped.csv\"\n",
    "cname = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "final_result = {**best_result.metrics, **best_result.config, **config,\n",
    "                \"checkpoint_path\": cname,\n",
    "                \"n_entropy_features\": best_result.config[\"n_features\"]}\n",
    "model, data_train = best_model(final_result, exp_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5241304-4736-431c-839d-179185e244c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_features = [\"log_value\"]\n",
    "\n",
    "path = config[\"model_path\"]\n",
    "\n",
    "scalerX = minmax_from_json(path+'Xscaler.json')\n",
    "scalerY = minmax_from_json(path+'Yscaler.json')    \n",
    "\n",
    "data_train = dataset(config[\"data_csv_train\"], y_features=y_features,\n",
    "                             scalerX=scalerX, scalerY=scalerY, keepXY=True,keep_features=[\"iupac_name\",\"family\",\"canonical_smiles\"])\n",
    "data_val = dataset(config[\"data_csv_val\"], y_features=y_features,\n",
    "                           scalerX=scalerX, scalerY=scalerY, keepXY=True,keep_features=[\"iupac_name\",\"family\",\"canonical_smiles\"])\n",
    "data_test = dataset(config[\"data_csv_test\"], y_features=y_features,\n",
    "                            scalerX=scalerX, scalerY=scalerY, keepXY=True,keep_features=[\"iupac_name\",\"family\",\"canonical_smiles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee3293-ccee-4ef9-aa5e-0bfd4b1c5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_error = True\n",
    "\n",
    "\n",
    "#a = [\"training\", \"validation\", \"test\"]\n",
    "#b = [data_train, data_val, data_test]\n",
    "a = [\"training\", ]\n",
    "b = [data_train, ]\n",
    "ps = []\n",
    "ks = []\n",
    "for dd, data in zip(a, b):\n",
    "\n",
    "    X = data.X_scaled\n",
    "    Y = data.Y_scaled\n",
    "    \n",
    "    y_pred = model(X)\n",
    "\n",
    "    Y = scalerY.inverse_transform(Y.detach().numpy())\n",
    "    y_pred = scalerY.inverse_transform(y_pred.detach().numpy())\n",
    "    \n",
    "    yy = np.squeeze(Y)\n",
    "    y_pred = np.squeeze(y_pred)\n",
    "\n",
    "    if exp_error:\n",
    "        error = np.abs((np.exp(y_pred) - np.exp(yy)) / np.exp(yy))\n",
    "    else:\n",
    "        error = np.mean(np.abs((y_pred - yy) / yy))  \n",
    "\n",
    "    xxx = X[:,1]#[ error >= error_max]\n",
    "    yyy = Y[:,0]#[ error >= error_max]\n",
    "    xerror = error#[ error >= error_max]      \n",
    "    \n",
    "    xx = X[:,1]#[ error < error_max]\n",
    "    yy = y_pred#[ error < error_max]\n",
    "    error = error#[ error < error_max]        \n",
    "    mean_error = np.mean(error)*100\n",
    "    median_error = np.median(error)*100        \n",
    "    rmse_error = np.sqrt(np.mean(error**2))\n",
    "   \n",
    "    \n",
    "    plt.plot( xxx, yyy, \"kx\", label=\"x\"+dd )\n",
    "    plt.plot( xx, yy, \".\", label=dd, alpha=1 )\n",
    "    #plt.plot( xx, error, \".\", label=dd )\n",
    "    #plt.plot( xxx, xerror, \".\", label=dd )\n",
    "    print(mean_error, median_error, rmse_error)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fecf08-cc31-4ea8-a126-f17e6948983a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cname = os.path.join(config[\"model_path\"], \"checkpoint.pt\")\n",
    "final_result = {**config,\n",
    "                \"checkpoint_path\": cname,\n",
    "                \"n_entropy_features\": config[\"n_features\"]}\n",
    "model, data_train = best_model(final_result, exp_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f9824-a842-4521-bf83-028fdc2fbc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4aa055-deb5-45ec-812e-0881c7b4fc29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
